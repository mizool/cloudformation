AWSTemplateFormatVersion: 2010-09-09
Description: This template deploys a Presto cluster connected to a cassandra database.

Parameters:
  networkStackName:
    Description: The name of an existing network stack in which Presto will be created.
    Type: String
    MinLength: 1

  coordinatorInstanceType:
    Description: The instance type used for the Presto coordinator.
    Type: String
    MinLength: 1
    Default: r5.large

  workerInstanceType:
    Description: The instance type used for the Presto workers.
    Type: String
    MinLength: 1
    Default: r5.large

  cassandraStackName:
    Description: The name of an existing Cassandra stack that Presto will get access to.
    Type: String
    MinLength: 1

  environmentName:
    Description: The name of the Presto environment that will be created.
    Type: String
    MinLength: 2
    AllowedPattern: "^[a-z0-9]*$"

  papertrailHost:
    Description: The host name of the papertrail log destination to use by the application.
    Type: String
    MinLength: 1

  papertrailPort:
    Description: The port of the papertrail log destination to use by the application.
    Type: Number
    MinValue: 1
    MaxValue: 65535

  numberOfWorkers:
    Description: How many worker nodes shall initially be started.
    Type: Number
    Default: 1
    MinValue: 1
    MaxValue: 100

  keyName:
    Description: The Key Manager key to be added to the list of allowed keys.
    Type: AWS::EC2::KeyPair::KeyName

  authorizedKeys:
    Description: A URL pointing to a list of authorized SSH keys to be appended as-is to ~/.ssh/authorized_keys.
    Type: String
    MinLength: 1

  internalDomain:
    Description: The internal DNS domain name to be used by the Presto cluster. If you run multiple clusters in the
      same VPC, the internal domain names must be distinct.
    Type: String
    MinLength: 1
    Default: presto

  usedAvailabilityZones:
    Description: The amount of availability zones over which to distribute the cluster.
    Type: Number
    MinValue: 1
    MaxValue: 3
    Default: 3

  influxHost:
    Type: String
    Default: ""

  influxPort:
    Type: Number
    Default: 8086

  influxDatabase:
    Type: String
    Default: ""

  influxTagStage:
    Type: String
    Default: ""

  influxTagProduct:
    Type: String
    Default: ""

  influxTagTeam:
    Type: String
    Default: ""

Conditions:
  useSecondAvailabilityZone:
    Fn::Or:
      - Fn::Equals: [ !Ref usedAvailabilityZones, 2 ]
      - Fn::Equals: [ !Ref usedAvailabilityZones, 3 ]

  useThirdAvailabilityZone:
    Fn::Equals: [ !Ref usedAvailabilityZones, 3 ]

Mappings:
  Regions:
    eu-central-1:
      AmazonLinux2Ami: ami-0f3a43fbf2d3899f7
    eu-west-1:
      AmazonLinux2Ami: ami-01f14919ba412de34

  Scripts:
    Telegraf:
      AddInstanceIdToTelegrafConf: |
        #!/bin/bash -xe
        export INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id)
        sed -i "/  instance =/c\  instance = \"${INSTANCE_ID}\"" /etc/telegraf/telegraf.conf

  Files:
    Telegraf:
      InfluxDbRepo: |
        [influxdb]
        name = InfluxDB Repository - RHEL
        baseurl = https://repos.influxdata.com/rhel/7/x86_64/stable/
        enabled = 1
        gpgcheck = 1
        gpgkey = https://repos.influxdata.com/influxdb.key

      TelegrafConf: |
        [global_tags]
          stage = "{{influxTagStage}}"
          product = "{{influxTagProduct}}"
          region = "${AWS::Region}"
          team = "{{influxTagTeam}}"
          instance =
        [agent]
          interval = "10s"
          round_interval = true
          metric_batch_size = 1000
          metric_buffer_limit = 10000
          collection_jitter = "0s"
          flush_interval = "10s"
          flush_jitter = "0s"
          precision = ""
          debug = false
          quiet = false
          logfile = ""
          hostname = ""
          omit_hostname = false
        [[outputs.influxdb]]
          urls = ["http://{{influxHost}}:{{influxPort}}"]
          database = "{{influxDatabase}}"
        [[inputs.cpu]]
          percpu = true
          totalcpu = true
          collect_cpu_time = false
          report_active = false
        [[inputs.disk]]
          ignore_fs = ["tmpfs", "devtmpfs", "devfs", "overlay", "aufs", "squashfs"]

Resources:
  coordinatorSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${AWS::StackName} Coordinator Security Group
      GroupDescription: !Sub ${AWS::StackName} Coordinator Security Group
      VpcId:
        Fn::ImportValue:
          !Sub "${networkStackName}-vpc"
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName} Coordinator Security Group

  workerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub ${AWS::StackName} Worker Security Group
      GroupDescription: !Sub ${AWS::StackName} Worker Security Group
      VpcId:
        Fn::ImportValue:
          !Sub "${networkStackName}-vpc"
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName} Worker Security Group

  bastionCoordinator22Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref coordinatorSecurityGroup
      IpProtocol: tcp
      FromPort: 22
      ToPort: 22
      SourceSecurityGroupId:
        Fn::ImportValue:
          !Sub "${networkStackName}-bastion-security-group"

  bastionWorker22Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref workerSecurityGroup
      IpProtocol: tcp
      FromPort: 22
      ToPort: 22
      SourceSecurityGroupId:
        Fn::ImportValue:
          !Sub "${networkStackName}-bastion-security-group"

  bastionCoordinator8080Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref coordinatorSecurityGroup
      IpProtocol: tcp
      FromPort: 8080
      ToPort: 8080
      SourceSecurityGroupId:
        Fn::ImportValue:
          !Sub "${networkStackName}-bastion-security-group"

  coordinatorWorker8080Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref workerSecurityGroup
      IpProtocol: tcp
      FromPort: 8080
      ToPort: 8080
      SourceSecurityGroupId: !Ref coordinatorSecurityGroup

  workerCoordinator8080Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref coordinatorSecurityGroup
      IpProtocol: tcp
      FromPort: 8080
      ToPort: 8080
      SourceSecurityGroupId: !Ref workerSecurityGroup

  workerWorker8080Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref workerSecurityGroup
      IpProtocol: tcp
      FromPort: 8080
      ToPort: 8080
      SourceSecurityGroupId: !Ref workerSecurityGroup

  coordinatorCassandra9042Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId:
        Fn::ImportValue:
          !Sub "${cassandraStackName}-security-group"
      IpProtocol: tcp
      FromPort: 9042
      ToPort: 9042
      SourceSecurityGroupId: !Ref coordinatorSecurityGroup

  workerCassandra9042Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId:
        Fn::ImportValue:
          !Sub "${cassandraStackName}-security-group"
      IpProtocol: tcp
      FromPort: 9042
      ToPort: 9042
      SourceSecurityGroupId: !Ref workerSecurityGroup

  coordinatorCassandra9160Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId:
        Fn::ImportValue:
          !Sub "${cassandraStackName}-security-group"
      IpProtocol: tcp
      FromPort: 9160
      ToPort: 9160
      SourceSecurityGroupId: !Ref coordinatorSecurityGroup

  workerCassandra9160Ingress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId:
        Fn::ImportValue:
          !Sub "${cassandraStackName}-security-group"
      IpProtocol: tcp
      FromPort: 9160
      ToPort: 9160
      SourceSecurityGroupId: !Ref workerSecurityGroup

  hostedZone:
    Type: AWS::Route53::HostedZone
    Properties:
      Name: !Ref internalDomain
      VPCs:
        - VPCId:
            Fn::ImportValue:
              !Sub "${networkStackName}-vpc"
          VPCRegion: !Ref "AWS::Region"

  prestoRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - "ec2.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Path: "/"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
      Policies:
        - PolicyName: "AllowAllPolicy"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - "route53:GetHostedZone"
                  - "route53:ListHostedZones"
                  - "route53:ListResourceRecordSets"
                  - "route53:ChangeResourceRecordSets"
                Resource: "*"

  instanceProfile:
    Type: "AWS::IAM::InstanceProfile"
    Properties:
      Path: "/"
      Roles:
        - !Ref prestoRole

  coordinatorLaunchConfiguration:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      ImageId: !FindInMap
        - Regions
        - !Ref AWS::Region
        - AmazonLinux2Ami
      KeyName: !Ref keyName
      InstanceType: !Ref coordinatorInstanceType
      IamInstanceProfile: !Ref instanceProfile
      SecurityGroups:
        - !Ref coordinatorSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash -x
          echo "${project.version} - ${timestamp}"
          echo "networkStackName: ${networkStackName}"
          echo "coordinatorInstanceType: ${coordinatorInstanceType}"
          echo "cassandraStackName: ${cassandraStackName}"
          echo "environmentName: ${environmentName}"
          echo "papertrailHost: ${papertrailHost}"
          echo "papertrailPort: ${papertrailPort}"
          echo "numberOfWorkers: ${numberOfWorkers}"
          echo "keyName: ${keyName}"
          echo "authorizedKeys: ${authorizedKeys}"
          echo "internalDomain: ${internalDomain}"
          echo "influxHost: ${influxHost}"
          echo "influxPort: ${influxPort}"
          echo "influxDatabase: ${influxDatabase}"
          echo "influxTagStage: ${influxTagStage}"
          echo "influxTagProduct: ${influxTagProduct}"
          echo "influxTagTeam: ${influxTagTeam}"
          yum update -y
          if [ -n "${influxHost}" ]
          then
              CONFIG_SET=setup_with_telegraf
          else
              CONFIG_SET=setup_without_telegraf
          fi
          /opt/aws/bin/cfn-init -v --configsets $CONFIG_SET --stack ${AWS::StackName} --resource coordinatorLaunchConfiguration --region ${AWS::Region}
          /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource coordinatorAutoScalingGroup --region ${AWS::Region}
    Metadata:
      AWS::CloudFormation::Init:
        configSets:
          basics:
            - "ssh_keys"
            - "ntp"
            - "haveged"
            - "openjdk8"
            - "aws_cli"
            - "cloud_watch_agent"
          telegraf:
            - "add_influxdb_repo"
            - "install_telegraf"
            - "configure_telegraf"
            - "start_telegraf"
          presto:
            - "register_dns"
            - "download_presto"
            - "configure_presto"
            - "setup_remote_syslog"
            - "start_presto"
            - "setup_presto_cli"
          setup_with_telegraf:
            - ConfigSet: "basics"
            - ConfigSet: "telegraf"
            - ConfigSet: "presto"
          setup_without_telegraf:
            - ConfigSet: "basics"
            - ConfigSet: "presto"
        ssh_keys:
          files:
            /home/ec2-user/.ssh/additional_keys:
              source: !Ref authorizedKeys
              owner: root
              group: root
              mode: "000644"
          commands:
            register_additional_keys:
              command: "cat /home/ec2-user/.ssh/additional_keys >> /home/ec2-user/.ssh/authorized_keys"
        ntp:
          packages:
            yum:
              ntp: []
          services:
            sysvinit:
              ntpd:
                enabled: true
                ensureRunning: true
                packages:
                  yum:
                    - ntp
        haveged:
          packages:
            rpm:
              haveged: "https://dl.fedoraproject.org/pub/archive/epel/7/x86_64/Packages/h/haveged-1.9.1-1.el7.x86_64.rpm"
          services:
            sysvinit:
              haveged:
                enabled: true
                ensureRunning: true
        openjdk8:
          packages:
            yum:
              java-1.8.0-openjdk: ["1.8.0.222.b10-0.amzn2.0.1"]
              java-1.8.0-openjdk-devel: ["1.8.0.222.b10-0.amzn2.0.1"]
        aws_cli:
          files:
            /home/ec2-user/.aws/config:
              content: !Sub |
                [default]
                region=${AWS::Region}
                output=json
              owner: ec2-user
              group: ec2-user
              mode: "000644"
        cloud_watch_agent:
          files:
            /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json:
              content: |
                {
                  "agent": {
                    "metrics_collection_interval": 60,
                    "run_as_user": "cwagent"
                  },
                  "metrics": {
                    "append_dimensions": {
                      "AutoScalingGroupName": "${aws:AutoScalingGroupName}",
                      "ImageId": "${aws:ImageId}",
                      "InstanceId": "${aws:InstanceId}",
                      "InstanceType": "${aws:InstanceType}"
                    },
                    "metrics_collected": {
                      "disk": {
                        "measurement": [
                          "used_percent"
                        ],
                        "metrics_collection_interval": 60,
                        "resources": [
                          "*"
                        ]
                      },
                      "mem": {
                        "measurement": [
                          "mem_used_percent"
                        ],
                        "metrics_collection_interval": 60
                      }
                    }
                  }
                }
              owner: root
              group: root
              mode: "000644"
          packages:
            yum:
              amazon-cloudwatch-agent: [ ]
          services:
            sysvinit:
              amazon-cloudwatch-agent:
                enabled: true
                ensureRunning: true
                packages:
                  yum:
                    - amazon-cloudwatch-agent
        register_dns:
          commands:
            01_create_change_batch:
              command: !Join
                - ""
                - - "echo '{\"Changes\": [{\"Action\": \"UPSERT\", \"ResourceRecordSet\": {\"Name\": \"coordinator."
                  - !Ref internalDomain
                  - "\", \"Type\": \"A\", \"TTL\": 60, \"ResourceRecords\": [{\"Value\": \"'$(hostname -I)'\"}]}}]}' > /home/ec2-user/change-batch.json"
            02_register_dns:
              command: !Join
                - ""
                - - "aws route53 change-resource-record-sets --hosted-zone-id "
                  - !Ref hostedZone
                  - " --change-batch file:///home/ec2-user/change-batch.json"
            03_remove_change_batch:
              command: "rm /home/ec2-user/change-batch.json"
            04_add_hosts_entry:
              command: !Join
                - ""
                - - "echo \"$(hostname -I) coordinator."
                  - !Ref internalDomain
                  - "\" >> /etc/hosts"
        add_influxdb_repo:
          files:
            /etc/yum.repos.d/influxdb.repo:
              content: !FindInMap ["Files", "Telegraf", "InfluxDbRepo"]
              owner: root
              group: root
              mode: "000644"
        install_telegraf:
          packages:
            yum:
              telegraf: []
        configure_telegraf:
          files:
            /etc/telegraf/telegraf.conf:
              content: !FindInMap ["Files", "Telegraf", "TelegrafConf"]
              context:
                influxHost: !Ref influxHost
                influxPort: !Ref influxPort
                influxDatabase: !Ref influxDatabase
                influxTagStage: !Ref influxTagStage
                influxTagProduct: !Ref influxTagProduct
                influxTagTeam: !Ref influxTagTeam
              owner: ec2-user
              group: ec2-user
              mode: "000644"
          commands:
            add_instance_id_to_telegraf_conf:
              command: !FindInMap ["Scripts", "Telegraf", "AddInstanceIdToTelegrafConf"]
        start_telegraf:
          services:
            sysvinit:
              telegraf:
                enabled: true
                ensureRunning: true
        download_presto:
          commands:
            download_presto:
              command: |
                #!/bin/bash -xe
                mkdir /opt/presto
                wget -q -O - "https://repo1.maven.org/maven2/io/prestosql/presto-server/326/presto-server-326.tar.gz" | tar xvzf - -C /opt/presto
                mv "/opt/presto/presto-server-326" /opt/presto/presto-server
                mkdir /var/presto
                mkdir /var/presto/data
        configure_presto:
          files:
            /opt/presto/presto-server/etc/node.properties:
              content: |
                node.data-dir=/var/presto/data
                node.environment={{nodeEnvironment}}
              context:
                nodeEnvironment: !Ref environmentName
              owner: root
              group: root
              mode: "000644"
            /opt/presto/presto-server/etc/jvm.config:
              content: |
                -server
                -XX:-UseBiasedLocking
                -XX:+UseG1GC
                -XX:G1HeapRegionSize=32M
                -XX:+ExplicitGCInvokesConcurrent
                -XX:OnOutOfMemoryError="kill -9 %p"
                -XX:+UseGCOverheadLimit
                -XX:ReservedCodeCacheSize=512M
                -Djdk.attach.allowAttachSelf=true
                -Djdk.nio.maxCachedBufferSize=2000000
                -Dcom.datastax.driver.NATIVE_TRANSPORT_MAX_FRAME_SIZE_IN_MB=1024
              owner: root
              group: root
              mode: "000644"
            /opt/presto/presto-server/etc/config.properties:
              content: |
                http-server.http.port=8080
                query.max-memory=50GB
                query.max-memory-per-node=1GB
                query.max-total-memory-per-node=2GB
                coordinator=true
                node-scheduler.include-coordinator=false
                discovery-server.enabled=true
                discovery.uri=http://{{discoveryUri}}:8080
              context:
                discoveryUri:
                  Fn::Join:
                    - ""
                    - - "coordinator."
                      - !Ref internalDomain
              owner: root
              group: root
              mode: "000644"
            /opt/presto/presto-server/etc/log.properties:
              content: |
                io.prestosql=WARN
              owner: root
              group: root
              mode: "000644"
            /opt/presto/presto-server/etc/catalog/cassandra.properties:
              content: |
                connector.name=cassandra
                cassandra.contact-points={{cassandraContactPoints}}
                cassandra.consistency-level=LOCAL_QUORUM
                cassandra.client.read-timeout=6m
              context:
                cassandraContactPoints:
                  Fn::ImportValue:
                    !Sub "${cassandraStackName}-seed-nodes"
              owner: root
              group: root
              mode: "000644"
          commands:
            patch_jvm_config:
              command: |
                export SYSTEM_MEMORY_IN_MB=`free -m | awk '/:/ {print $2;exit}'`
                export HEAP_SIZE=`expr $SYSTEM_MEMORY_IN_MB / 4 \* 3`
                echo "-Xmx${HEAP_SIZE}M" >> /opt/presto/presto-server/etc/jvm.config
            patch_node_properties:
              command: |
                export INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id)
                echo "node.id=${INSTANCE_ID}" >> /opt/presto/presto-server/etc/node.properties
        setup_remote_syslog:
          files:
            /etc/log_files.yml:
              content: |
                files:
                  - /var/presto/data/var/log/presto
                destination:
                  host: {{papertrailHost}}
                  port: {{papertrailPort}}
                  protocol: tls
              context:
                papertrailHost: !Ref papertrailHost
                papertrailPort: !Ref papertrailPort
              owner: root
              group: root
              mode: "000644"
          commands:
            01_download_remote_syslog:
              command: |
                #!/bin/bash -xe
                wget -q -O - "https://github.com/papertrail/remote_syslog2/releases/download/v0.20/remote_syslog_linux_amd64.tar.gz" | tar xvzf - -C /etc
            02_patch_log_files_yml:
              command: |
                export INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id)
                echo "hostname: presto-coordinator-${INSTANCE_ID}" >> /etc/log_files.yml
        start_presto:
          files:
            /home/ec2-user/run-presto.sh:
              content: !Sub |
                #!/bin/bash -x
                /etc/remote_syslog/remote_syslog
                /opt/presto/presto-server/bin/launcher run
                shutdown -h now
              owner: root
              group: root
              mode: "000755"
          commands:
            run_presto:
              command: |
                mkdir /var/presto/data/var
                mkdir /var/presto/data/var/log
                touch /var/presto/data/var/log/presto
                nohup /home/ec2-user/run-presto.sh &>/var/presto/data/var/log/presto &
        setup_presto_cli:
          commands:
            setup_presto_cli:
              command: |
                #!/bin/bash -xe
                mkdir /opt/presto/presto-cli
                wget -O /opt/presto/presto-cli/presto-cli-executable.jar "https://repo1.maven.org/maven2/io/prestosql/presto-cli/326/presto-cli-326-executable.jar"
                chmod +x /opt/presto/presto-cli/presto-cli-executable.jar
                ln -s /opt/presto/presto-cli/presto-cli-executable.jar /bin/presto

  workerLaunchConfiguration:
    Type: AWS::AutoScaling::LaunchConfiguration
    Properties:
      ImageId: !FindInMap
        - Regions
        - !Ref AWS::Region
        - AmazonLinux2Ami
      KeyName: !Ref keyName
      InstanceType: !Ref workerInstanceType
      IamInstanceProfile: !Ref instanceProfile
      SecurityGroups:
        - !Ref workerSecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash -x
          echo "${project.version} - ${timestamp}"
          echo "networkStackName: ${networkStackName}"
          echo "workerInstanceType: ${workerInstanceType}"
          echo "cassandraStackName: ${cassandraStackName}"
          echo "environmentName: ${environmentName}"
          echo "papertrailHost: ${papertrailHost}"
          echo "papertrailPort: ${papertrailPort}"
          echo "numberOfWorkers: ${numberOfWorkers}"
          echo "keyName: ${keyName}"
          echo "authorizedKeys: ${authorizedKeys}"
          echo "internalDomain: ${internalDomain}"
          echo "influxHost: ${influxHost}"
          echo "influxPort: ${influxPort}"
          echo "influxDatabase: ${influxDatabase}"
          echo "influxTagStage: ${influxTagStage}"
          echo "influxTagProduct: ${influxTagProduct}"
          echo "influxTagTeam: ${influxTagTeam}"
          yum update -y
          if [ -n "${influxHost}" ]
          then
              CONFIG_SET=setup_with_telegraf
          else
              CONFIG_SET=setup_without_telegraf
          fi
          /opt/aws/bin/cfn-init -v --configsets $CONFIG_SET --stack ${AWS::StackName} --resource workerLaunchConfiguration --region ${AWS::Region}
          /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource workerAutoScalingGroup --region ${AWS::Region}
    Metadata:
      AWS::CloudFormation::Init:
        configSets:
          basics:
            - "ssh_keys"
            - "ntp"
            - "haveged"
            - "openjdk8"
            - "cloud_watch_agent"
          telegraf:
            - "add_influxdb_repo"
            - "install_telegraf"
            - "configure_telegraf"
            - "start_telegraf"
          presto:
            - "download_presto"
            - "configure_presto"
            - "setup_remote_syslog"
            - "start_presto"
          setup_with_telegraf:
            - ConfigSet: "basics"
            - ConfigSet: "telegraf"
            - ConfigSet: "presto"
          setup_without_telegraf:
            - ConfigSet: "basics"
            - ConfigSet: "presto"
        ssh_keys:
          files:
            /home/ec2-user/.ssh/additional_keys:
              source: !Ref authorizedKeys
              owner: root
              group: root
              mode: "000644"
          commands:
            register_additional_keys:
              command: "cat /home/ec2-user/.ssh/additional_keys >> /home/ec2-user/.ssh/authorized_keys"
        ntp:
          packages:
            yum:
              ntp: []
          services:
            sysvinit:
              ntpd:
                enabled: true
                ensureRunning: true
                packages:
                  yum:
                    - ntp
        haveged:
          packages:
            rpm:
              haveged: "https://dl.fedoraproject.org/pub/archive/epel/7/x86_64/Packages/h/haveged-1.9.1-1.el7.x86_64.rpm"
          services:
            sysvinit:
              haveged:
                enabled: true
                ensureRunning: true
        openjdk8:
          packages:
            yum:
              java-1.8.0-openjdk: ["1.8.0.222.b10-0.amzn2.0.1"]
              java-1.8.0-openjdk-devel: ["1.8.0.222.b10-0.amzn2.0.1"]
        cloud_watch_agent:
          files:
            /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json:
              content: |
                {
                  "agent": {
                    "metrics_collection_interval": 60,
                    "run_as_user": "cwagent"
                  },
                  "metrics": {
                    "append_dimensions": {
                      "AutoScalingGroupName": "${aws:AutoScalingGroupName}",
                      "ImageId": "${aws:ImageId}",
                      "InstanceId": "${aws:InstanceId}",
                      "InstanceType": "${aws:InstanceType}"
                    },
                    "metrics_collected": {
                      "disk": {
                        "measurement": [
                          "used_percent"
                        ],
                        "metrics_collection_interval": 60,
                        "resources": [
                          "*"
                        ]
                      },
                      "mem": {
                        "measurement": [
                          "mem_used_percent"
                        ],
                        "metrics_collection_interval": 60
                      }
                    }
                  }
                }
              owner: root
              group: root
              mode: "000644"
          packages:
            yum:
              amazon-cloudwatch-agent: [ ]
          services:
            sysvinit:
              amazon-cloudwatch-agent:
                enabled: true
                ensureRunning: true
                packages:
                  yum:
                    - amazon-cloudwatch-agent
        add_influxdb_repo:
          files:
            /etc/yum.repos.d/influxdb.repo:
              content: !FindInMap ["Files", "Telegraf", "InfluxDbRepo"]
              owner: root
              group: root
              mode: "000644"
        install_telegraf:
          packages:
            yum:
              telegraf: []
        configure_telegraf:
          files:
            /etc/telegraf/telegraf.conf:
              content: !FindInMap ["Files", "Telegraf", "TelegrafConf"]
              context:
                influxHost: !Ref influxHost
                influxPort: !Ref influxPort
                influxDatabase: !Ref influxDatabase
                influxTagStage: !Ref influxTagStage
                influxTagProduct: !Ref influxTagProduct
                influxTagTeam: !Ref influxTagTeam
              owner: ec2-user
              group: ec2-user
              mode: "000644"
          commands:
            add_instance_id_to_telegraf_conf:
              command: !FindInMap ["Scripts", "Telegraf", "AddInstanceIdToTelegrafConf"]
        start_telegraf:
          services:
            sysvinit:
              telegraf:
                enabled: true
                ensureRunning: true
        download_presto:
          commands:
            download_presto:
              command: |
                #!/bin/bash -xe
                mkdir /opt/presto
                wget -q -O - "https://repo1.maven.org/maven2/io/prestosql/presto-server/326/presto-server-326.tar.gz" | tar xvzf - -C /opt/presto
                mv "/opt/presto/presto-server-326" /opt/presto/presto-server
                mkdir /var/presto
                mkdir /var/presto/data
        configure_presto:
          files:
            /opt/presto/presto-server/etc/node.properties:
              content: |
                node.data-dir=/var/presto/data
                node.environment={{nodeEnvironment}}
              context:
                nodeEnvironment: !Ref environmentName
              owner: root
              group: root
              mode: "000644"
            /opt/presto/presto-server/etc/jvm.config:
              content: |
                -server
                -XX:-UseBiasedLocking
                -XX:+UseG1GC
                -XX:G1HeapRegionSize=32M
                -XX:+ExplicitGCInvokesConcurrent
                -XX:OnOutOfMemoryError="kill -9 %p"
                -XX:+UseGCOverheadLimit
                -XX:ReservedCodeCacheSize=512M
                -Djdk.attach.allowAttachSelf=true
                -Djdk.nio.maxCachedBufferSize=2000000
                -Dcom.datastax.driver.NATIVE_TRANSPORT_MAX_FRAME_SIZE_IN_MB=1024
              owner: root
              group: root
              mode: "000644"
            /opt/presto/presto-server/etc/config.properties:
              content: |
                http-server.http.port=8080
                query.max-memory=50GB
                query.max-memory-per-node=1GB
                query.max-total-memory-per-node=2GB
                coordinator=false
                discovery.uri=http://{{discoveryUri}}:8080
              context:
                discoveryUri:
                  Fn::Join:
                    - ""
                    - - "coordinator."
                      - !Ref internalDomain
              owner: root
              group: root
              mode: "000644"
            /opt/presto/presto-server/etc/log.properties:
              content: |
                io.prestosql=WARN
              owner: root
              group: root
              mode: "000644"
            /opt/presto/presto-server/etc/catalog/cassandra.properties:
              content: |
                connector.name=cassandra
                cassandra.contact-points={{cassandraContactPoints}}
                cassandra.consistency-level=LOCAL_QUORUM
                cassandra.client.read-timeout=6m
              context:
                cassandraContactPoints:
                  Fn::ImportValue:
                    !Sub "${cassandraStackName}-seed-nodes"
              owner: root
              group: root
              mode: "000644"
          commands:
            patch_jvm_config:
              command: |
                export SYSTEM_MEMORY_IN_MB=`free -m | awk '/:/ {print $2;exit}'`
                export HEAP_SIZE=`expr $SYSTEM_MEMORY_IN_MB / 4 \* 3`
                echo "-Xmx${HEAP_SIZE}M" >> /opt/presto/presto-server/etc/jvm.config
            patch_node_properties:
              command: |
                export INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id)
                echo "node.id=${INSTANCE_ID}" >> /opt/presto/presto-server/etc/node.properties
        setup_remote_syslog:
          files:
            /etc/log_files.yml:
              content: |
                files:
                  - /var/presto/data/var/log/presto
                destination:
                  host: {{papertrailHost}}
                  port: {{papertrailPort}}
                  protocol: tls
              context:
                papertrailHost: !Ref papertrailHost
                papertrailPort: !Ref papertrailPort
              owner: root
              group: root
              mode: "000644"
          commands:
            01_download_remote_syslog:
              command: |
                #!/bin/bash -xe
                wget -q -O - "https://github.com/papertrail/remote_syslog2/releases/download/v0.20/remote_syslog_linux_amd64.tar.gz" | tar xvzf - -C /etc
            02_patch_log_files_yml:
              command: |
                export INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id)
                echo "hostname: presto-worker-${INSTANCE_ID}" >> /etc/log_files.yml
        start_presto:
          files:
            /home/ec2-user/run-presto.sh:
              content: !Sub |
                #!/bin/bash -x
                /etc/remote_syslog/remote_syslog
                /opt/presto/presto-server/bin/launcher run
                shutdown -h now
              owner: root
              group: root
              mode: "000755"
          commands:
            run_presto:
              command: |
                mkdir /var/presto/data/var
                mkdir /var/presto/data/var/log
                touch /var/presto/data/var/log/presto
                nohup /home/ec2-user/run-presto.sh &>/var/presto/data/var/log/presto &

  coordinatorAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      LaunchConfigurationName: !Ref coordinatorLaunchConfiguration
      MaxSize: 2
      MinSize: 1
      DesiredCapacity: 1
      VPCZoneIdentifier:
        - Fn::Select:
            - 0
            - Fn::Split:
                - ","
                - Fn::ImportValue:
                    !Sub "${networkStackName}-private-subnets"
        - Fn::If:
          - useSecondAvailabilityZone
          - Fn::Select:
            - 1
            - Fn::Split:
                - ","
                - Fn::ImportValue:
                    !Sub "${networkStackName}-private-subnets"
          - !Ref AWS::NoValue
        - Fn::If:
          - useThirdAvailabilityZone
          - Fn::Select:
            - 2
            - Fn::Split:
                - ","
                - Fn::ImportValue:
                    !Sub "${networkStackName}-private-subnets"
          - !Ref AWS::NoValue
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName} Coordinator
          PropagateAtLaunch: true
    CreationPolicy:
      ResourceSignal:
        Timeout: PT10M
    UpdatePolicy:
      AutoScalingRollingUpdate:
        MinInstancesInService: 1
        WaitOnResourceSignals: true

  workerAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      LaunchConfigurationName: !Ref workerLaunchConfiguration
      MaxSize: 100
      MinSize: 1
      DesiredCapacity: !Ref numberOfWorkers
      VPCZoneIdentifier:
        - Fn::Select:
            - 0
            - Fn::Split:
                - ","
                - Fn::ImportValue:
                    !Sub "${networkStackName}-private-subnets"
        - Fn::If:
          - useSecondAvailabilityZone
          - Fn::Select:
            - 1
            - Fn::Split:
                - ","
                - Fn::ImportValue:
                    !Sub "${networkStackName}-private-subnets"
          - !Ref AWS::NoValue
        - Fn::If:
          - useThirdAvailabilityZone
          - Fn::Select:
            - 2
            - Fn::Split:
                - ","
                - Fn::ImportValue:
                    !Sub "${networkStackName}-private-subnets"
          - !Ref AWS::NoValue
      Tags:
        - Key: Name
          Value: !Sub ${AWS::StackName} Worker
          PropagateAtLaunch: true
    CreationPolicy:
      ResourceSignal:
        Timeout: PT10M
    UpdatePolicy:
      AutoScalingRollingUpdate:
        MinInstancesInService: 1
        WaitOnResourceSignals: true

Outputs:
  SecurityGroup:
    Description: The presto coordinator security group, so that other security groups can allow ingress from themselves
      to it.
    Value: !Ref coordinatorSecurityGroup
    Export:
      Name: !Sub ${AWS::StackName}-security-group

  DnsName:
    Description: The DNS name under which the presto coordinator is reachable.
    Value:
      Fn::Join:
        - ""
        - - "coordinator."
          - !Ref internalDomain
    Export:
      Name: !Sub ${AWS::StackName}-dns-name
